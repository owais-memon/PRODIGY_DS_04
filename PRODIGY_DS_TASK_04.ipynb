{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07eb53a2-05c7-46f2-b066-1f1f28e6eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\OWAIS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWAIS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|██████████| 1000/1000 [00:00<00:00, 87058.49it/s]\n",
      "C:\\Users\\OWAIS\\AppData\\Local\\Temp\\ipykernel_27880\\702146144.py:63: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"created_at\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring sentiment...\n",
      "✅ Analysis complete. Results saved to: sentiment_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_URL = \"https://raw.githubusercontent.com/luminati-io/Social-media-dataset-samples/main/Facebook-datasets.csv\"\n",
    "OUTPUT_DIR = \"sentiment_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_URL, low_memory=False)\n",
    "\n",
    "# --- COLUMN DETECTION ---\n",
    "text_candidates = [c for c in df.columns if any(k in c.lower() for k in [\"comment\", \"text\", \"post\", \"message\", \"description\"])]\n",
    "if not text_candidates:\n",
    "    raise ValueError(\"No text-like column found.\")\n",
    "text_col = text_candidates[0]\n",
    "\n",
    "date_candidates = [c for c in df.columns if any(k in c.lower() for k in [\"date\", \"time\", \"created\"])]\n",
    "date_col = date_candidates[0] if date_candidates else None\n",
    "\n",
    "# --- TEXT PREPROCESSING ---\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if pd.notna(text) else \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing text...\")\n",
    "\n",
    "# ✅ FIX: Ensure we’re working with a Series\n",
    "text_series = df[text_col]\n",
    "if isinstance(text_series, pd.DataFrame):\n",
    "    text_series = text_series.astype(str).agg(\" \".join, axis=1)\n",
    "else:\n",
    "    text_series = text_series.astype(str)\n",
    "\n",
    "texts_raw = text_series.tolist()\n",
    "df[\"text_clean\"] = [preprocess_text(x) for x in tqdm(texts_raw, desc=\"Cleaning\")]\n",
    "\n",
    "# --- DATE PARSING ---\n",
    "if date_col:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"created_at\"] = pd.NaT\n",
    "\n",
    "df = df[df[\"text_clean\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# --- SENTIMENT SCORING ---\n",
    "print(\"Scoring sentiment...\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "scores = df[\"text_clean\"].apply(sid.polarity_scores)\n",
    "scores_df = pd.DataFrame(list(scores))\n",
    "df = pd.concat([df, scores_df], axis=1)\n",
    "\n",
    "df[\"sentiment\"] = df[\"compound\"].apply(lambda c: \"positive\" if c >= 0.05 else \"negative\" if c <= -0.05 else \"neutral\")\n",
    "\n",
    "# --- AGGREGATIONS ---\n",
    "sentiment_counts = df[\"sentiment\"].value_counts().reset_index(name=\"count\")\n",
    "sentiment_counts.to_csv(f\"{OUTPUT_DIR}/sentiment_counts.csv\", index=False)\n",
    "\n",
    "if df[\"created_at\"].notna().any():\n",
    "    df[\"date\"] = df[\"created_at\"].dt.date\n",
    "    time_agg = df.groupby(\"date\")[\"compound\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "    time_agg.to_csv(f\"{OUTPUT_DIR}/time_agg.csv\", index=False)\n",
    "\n",
    "    daily_sent = df.groupby([\"date\", \"sentiment\"]).size().unstack(fill_value=0).reset_index()\n",
    "    daily_sent.to_csv(f\"{OUTPUT_DIR}/daily_sentiment_counts.csv\", index=False)\n",
    "else:\n",
    "    time_agg = None\n",
    "    daily_sent = None\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Sentiment distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=\"sentiment\", y=\"count\", data=sentiment_counts)\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/sentiment_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# Compound score distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.violinplot(x=\"sentiment\", y=\"compound\", data=df)\n",
    "plt.title(\"Compound Score by Sentiment\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/compound_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# Time series plots\n",
    "if time_agg is not None:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.lineplot(x=\"date\", y=\"mean\", data=time_agg)\n",
    "    plt.title(\"Daily Average Sentiment\")\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/daily_compound_mean.png\")\n",
    "    plt.close()\n",
    "\n",
    "    daily_sent.set_index(\"date\").plot.area(figsize=(10,5), alpha=0.6)\n",
    "    plt.title(\"Daily Sentiment Counts\")\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/daily_sentiment_stacked.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Word cloud\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "wordcloud = WordCloud(width=1200, height=600, background_color=\"white\", stopwords=stop_words).generate(\" \".join(df[\"text_clean\"]))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/wordcloud.png\")\n",
    "plt.close()\n",
    "\n",
    "# Top words\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def get_top_words(texts, n=30):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        tokens = tokenizer.tokenize(t)\n",
    "        for tk in tokens:\n",
    "            if tk.isalpha() and tk not in stop_words:\n",
    "                counter[tk] += 1\n",
    "    return counter.most_common(n)\n",
    "\n",
    "top_words = get_top_words(df[\"text_clean\"].tolist())\n",
    "pd.DataFrame(top_words, columns=[\"word\", \"count\"]).to_csv(f\"{OUTPUT_DIR}/top_words.csv\", index=False)\n",
    "\n",
    "print(\"✅ Analysis complete. Results saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839e899-252b-4ed0-a3a6-057b5f70740c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
